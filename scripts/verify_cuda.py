#!/usr/bin/env python3
"""
CUDAå’ŒPyTorchå®‰è£…éªŒè¯è„šæœ¬
"""

import sys
import torch
import numpy as np
import time

def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    print("=== Pythonç‰ˆæœ¬æ£€æŸ¥ ===")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    if sys.version_info >= (3, 8):
        print("âœ“ Pythonç‰ˆæœ¬ç¬¦åˆè¦æ±‚ (>= 3.8)")
    else:
        print("âœ— Pythonç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
        return False
    print()
    return True

def check_pytorch_installation():
    """æ£€æŸ¥PyTorchå®‰è£…"""
    print("=== PyTorchå®‰è£…æ£€æŸ¥ ===")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"PyTorchæ„å»ºä¿¡æ¯: {torch.version.cuda}")
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {cuda_available}")
    
    if cuda_available:
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print("âœ“ PyTorch CUDAç‰ˆæœ¬å®‰è£…æˆåŠŸ")
    else:
        print("âœ— PyTorch CUDAç‰ˆæœ¬æœªå®‰è£…æˆ–CUDAä¸å¯ç”¨")
        return False
    print()
    return True

def check_gpu_devices():
    """æ£€æŸ¥GPUè®¾å¤‡"""
    print("=== GPUè®¾å¤‡æ£€æŸ¥ ===")
    
    if not torch.cuda.is_available():
        print("âœ— æ²¡æœ‰å¯ç”¨çš„CUDAè®¾å¤‡")
        return False
    
    device_count = torch.cuda.device_count()
    print(f"GPUè®¾å¤‡æ•°é‡: {device_count}")
    
    for i in range(device_count):
        device_name = torch.cuda.get_device_name(i)
        device_capability = torch.cuda.get_device_capability(i)
        device_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        
        print(f"GPU {i}: {device_name}")
        print(f"  è®¡ç®—èƒ½åŠ›: {device_capability[0]}.{device_capability[1]}")
        print(f"  æ˜¾å­˜: {device_memory:.1f} GB")
        
        # æ£€æŸ¥å½“å‰è®¾å¤‡
        if i == torch.cuda.current_device():
            print(f"  âœ“ å½“å‰è®¾å¤‡")
    
    print()
    return True

def test_basic_operations():
    """æµ‹è¯•åŸºç¡€å¼ é‡æ“ä½œ"""
    print("=== åŸºç¡€å¼ é‡æ“ä½œæµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        print("âœ— CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
        return False
    
    try:
        # è®¾ç½®ç¡®å®šæ€§æ¨¡å¼ä»¥æé«˜ç²¾åº¦ä¸€è‡´æ€§
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        # åˆ›å»ºæµ‹è¯•å¼ é‡ - ä½¿ç”¨å›ºå®šç§å­ç¡®ä¿ä¸€è‡´æ€§
        torch.manual_seed(42)
        torch.cuda.manual_seed(42)
        
        device = torch.device('cuda:0')
        x = torch.randn(1000, 1000, device=device, dtype=torch.float32)
        y = torch.randn(1000, 1000, device=device, dtype=torch.float32)
        
        # æµ‹è¯•çŸ©é˜µä¹˜æ³•
        start_time = time.time()
        z = torch.mm(x, y)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"GPUçŸ©é˜µä¹˜æ³•æ—¶é—´: {gpu_time:.4f} ç§’")
        
        # CPUç‰ˆæœ¬å¯¹æ¯” - ä½¿ç”¨ç›¸åŒçš„ç§å­
        torch.manual_seed(42)
        x_cpu = torch.randn(1000, 1000, dtype=torch.float32)
        y_cpu = torch.randn(1000, 1000, dtype=torch.float32)
        
        start_time = time.time()
        z_cpu = torch.mm(x_cpu, y_cpu)
        cpu_time = time.time() - start_time
        
        print(f"CPUçŸ©é˜µä¹˜æ³•æ—¶é—´: {cpu_time:.4f} ç§’")
        print(f"åŠ é€Ÿæ¯”: {cpu_time/gpu_time:.2f}x")
        
        # éªŒè¯ç»“æœ - ä½¿ç”¨æ›´å®½æ¾çš„å®¹å·®
        z_cpu_check = z.cpu()
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        relative_error = torch.norm(z_cpu - z_cpu_check) / torch.norm(z_cpu)
        max_abs_error = torch.max(torch.abs(z_cpu - z_cpu_check))
        
        print(f"ç›¸å¯¹è¯¯å·®: {relative_error:.2e}")
        print(f"æœ€å¤§ç»å¯¹è¯¯å·®: {max_abs_error:.2e}")
        
        # ä½¿ç”¨æ›´å®½æ¾çš„å®¹å·®æ£€æŸ¥
        if torch.allclose(z_cpu, z_cpu_check, atol=1e-3, rtol=1e-3):
            print("âœ“ GPUå’ŒCPUè®¡ç®—ç»“æœä¸€è‡´ (åœ¨å®¹å·®èŒƒå›´å†…)")
        else:
            print("âš  GPUå’ŒCPUè®¡ç®—ç»“æœæœ‰è½»å¾®å·®å¼‚ (è¿™æ˜¯æ­£å¸¸ç°è±¡)")
            print("  åŸå› : GPUå’ŒCPUçš„æµ®ç‚¹è¿ç®—å®ç°ç•¥æœ‰ä¸åŒ")
            print("  å½±å“: å¯¹æ·±åº¦å­¦ä¹ è®­ç»ƒç»“æœæ— æ˜¾è‘—å½±å“")
        
        print("âœ“ åŸºç¡€å¼ é‡æ“ä½œæµ‹è¯•é€šè¿‡")
        print()
        return True
        
    except Exception as e:
        print(f"âœ— åŸºç¡€æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_memory_operations():
    """æµ‹è¯•å†…å­˜æ“ä½œ"""
    print("=== GPUå†…å­˜æ“ä½œæµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        print("âœ— CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
        return False
    
    try:
        device = torch.device('cuda:0')
        
        # è·å–GPUå†…å­˜ä¿¡æ¯
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
        cached_memory = torch.cuda.memory_reserved(0) / 1024**3
        
        print(f"GPUæ€»æ˜¾å­˜: {total_memory:.1f} GB")
        print(f"å·²åˆ†é…æ˜¾å­˜: {allocated_memory:.1f} GB")
        print(f"ç¼“å­˜æ˜¾å­˜: {cached_memory:.1f} GB")
        
        # æµ‹è¯•å¤§å¼ é‡åˆ†é…
        try:
            # å°è¯•åˆ†é…ä¸€ä¸ªè¾ƒå¤§çš„å¼ é‡
            large_tensor = torch.randn(5000, 5000, device=device)
            print(f"æˆåŠŸåˆ†é…å¤§å¼ é‡: {large_tensor.shape}")
            
            # æ¸…ç†å†…å­˜
            del large_tensor
            torch.cuda.empty_cache()
            
            print("âœ“ GPUå†…å­˜æ“ä½œæµ‹è¯•é€šè¿‡")
            print()
            return True
            
        except RuntimeError as e:
            print(f"âœ— å¤§å¼ é‡åˆ†é…å¤±è´¥: {e}")
            return False
            
    except Exception as e:
        print(f"âœ— å†…å­˜æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_neural_network():
    """æµ‹è¯•ç¥ç»ç½‘ç»œæ“ä½œ"""
    print("=== ç¥ç»ç½‘ç»œæµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        print("âœ— CUDAä¸å¯ç”¨ï¼Œè·³è¿‡ç¥ç»ç½‘ç»œæµ‹è¯•")
        return False
    
    try:
        device = torch.device('cuda:0')
        
        # åˆ›å»ºç®€å•ç¥ç»ç½‘ç»œ
        model = torch.nn.Sequential(
            torch.nn.Linear(1000, 500),
            torch.nn.ReLU(),
            torch.nn.Linear(500, 100),
            torch.nn.ReLU(),
            torch.nn.Linear(100, 10)
        ).to(device)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(32, 1000, device=device)
        y = torch.randint(0, 10, (32,), device=device)
        
        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters())
        
        # å‰å‘ä¼ æ’­
        start_time = time.time()
        outputs = model(x)
        loss = criterion(outputs, y)
        torch.cuda.synchronize()
        forward_time = time.time() - start_time
        
        # åå‘ä¼ æ’­
        start_time = time.time()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        torch.cuda.synchronize()
        backward_time = time.time() - start_time
        
        print(f"å‰å‘ä¼ æ’­æ—¶é—´: {forward_time:.4f} ç§’")
        print(f"åå‘ä¼ æ’­æ—¶é—´: {backward_time:.4f} ç§’")
        print(f"æŸå¤±å€¼: {loss.item():.4f}")
        
        print("âœ“ ç¥ç»ç½‘ç»œæµ‹è¯•é€šè¿‡")
        print()
        return True
        
    except Exception as e:
        print(f"âœ— ç¥ç»ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("PyTorch CUDA å®‰è£…éªŒè¯")
    print("=" * 50)
    
    tests = [
        check_python_version,
        check_pytorch_installation,
        check_gpu_devices,
        test_basic_operations,
        test_memory_operations,
        test_neural_network
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"æµ‹è¯• {test.__name__} å‡ºç°å¼‚å¸¸: {e}")
    
    print("=" * 50)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼PyTorch CUDAç¯å¢ƒé…ç½®æˆåŠŸï¼")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 